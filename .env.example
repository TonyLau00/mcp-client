# MCP Client Environment Variables

# ─── MCP Server ──────────────────────────────────────────────────
# MCP Server URL (tron-mcp-server SSE endpoint)
VITE_MCP_SERVER_URL=http://localhost:3100

# ─── LLM Provider ───────────────────────────────────────────────
# Active provider: openai | claude | deepseek | gemini | ollama | openrouter | custom
VITE_LLM_PROVIDER=deepseek

# ─── OpenAI ──────────────────────────────────────────────────────
VITE_OPENAI_API_KEY=
VITE_OPENAI_BASE_URL=https://api.openai.com/v1
VITE_OPENAI_MODEL=gpt-4o

# ─── Anthropic Claude ────────────────────────────────────────────
VITE_CLAUDE_API_KEY=
VITE_CLAUDE_BASE_URL=https://api.anthropic.com
VITE_CLAUDE_MODEL=claude-sonnet-4-20250514

# ─── DeepSeek ────────────────────────────────────────────────────
VITE_DEEPSEEK_API_KEY=
VITE_DEEPSEEK_BASE_URL=https://api.deepseek.com
VITE_DEEPSEEK_MODEL=deepseek-chat

# ─── Google Gemini ───────────────────────────────────────────────
VITE_GEMINI_API_KEY=
VITE_GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
VITE_GEMINI_MODEL=gemini-2.0-flash

# ─── Ollama (Local) ─────────────────────────────────────────────
# No API key needed for local Ollama
VITE_OLLAMA_BASE_URL=http://localhost:11434
VITE_OLLAMA_MODEL=qwen2.5:14b

# ─── OpenRouter ──────────────────────────────────────────────────
VITE_OPENROUTER_API_KEY=
VITE_OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
VITE_OPENROUTER_MODEL=anthropic/claude-sonnet-4

# ─── Custom OpenAI-Compatible ────────────────────────────────────
# For any OpenAI-compatible API (vLLM, LiteLLM, LMStudio, etc.)
VITE_CUSTOM_API_KEY=
VITE_CUSTOM_BASE_URL=http://localhost:8000/v1
VITE_CUSTOM_MODEL=default

